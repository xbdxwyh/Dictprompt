{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ce39a6",
   "metadata": {},
   "source": [
    "### Pretrain in Dictprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1114477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-5\n",
    "wd = 1e-6\n",
    "batch_size = 8\n",
    "epoch = 5\n",
    "#path = \"wordgamesBERTbase3kGM\"\n",
    "template_id = 7\n",
    "path = \"wgsBERTbase3kT{}\".format(template_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e76692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"F:\\\\preTrainedModels\\\\bert-base-uncased\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4dc25",
   "metadata": {},
   "source": [
    "# 1.0 load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cf3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db63d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('WordGameData/train_single_wordgames_ox3k.json')\n",
    "val = pd.read_json('WordGameData/val_single_wordgames_ox3k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79188d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mean</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20662</th>\n",
       "      <td>proceed</td>\n",
       "      <td>to continue doing sth that has already been st...</td>\n",
       "      <td>We're not sure whether we still want to proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9356</th>\n",
       "      <td>extension</td>\n",
       "      <td>a new part that is added to a building</td>\n",
       "      <td>a planned two-storey extension to the hospital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6533</th>\n",
       "      <td>cup</td>\n",
       "      <td>to make your hands into the shape of a bowl</td>\n",
       "      <td>She held the bird gently in cupped hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>point</td>\n",
       "      <td>a particular detail or fact</td>\n",
       "      <td>Here are the main points of the news.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15999</th>\n",
       "      <td>love</td>\n",
       "      <td>to have very strong feelings of affection for sb</td>\n",
       "      <td>If you love each other, why not get married?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18702</th>\n",
       "      <td>pack</td>\n",
       "      <td>to do a lot of things in a limited period of time</td>\n",
       "      <td>You seem to have packed a lot into your life!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>deny</td>\n",
       "      <td>to refuse to admit or accept sth</td>\n",
       "      <td>She denied all knowledge of the incident.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14493</th>\n",
       "      <td>itself</td>\n",
       "      <td>used to emphasize an animal, a thing, etc.</td>\n",
       "      <td>The village itself is pretty, but the surroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>free</td>\n",
       "      <td>not attached to sth or trapped by sth</td>\n",
       "      <td>Pull gently on the free end of the rope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>elementary</td>\n",
       "      <td>of the most basic kind</td>\n",
       "      <td>the elementary laws of economics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16015 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                               mean  \\\n",
       "20662     proceed  to continue doing sth that has already been st...   \n",
       "9356    extension             a new part that is added to a building   \n",
       "6533          cup        to make your hands into the shape of a bowl   \n",
       "19993       point                        a particular detail or fact   \n",
       "15999        love   to have very strong feelings of affection for sb   \n",
       "...           ...                                                ...   \n",
       "18702        pack  to do a lot of things in a limited period of time   \n",
       "7130         deny                   to refuse to admit or accept sth   \n",
       "14493      itself         used to emphasize an animal, a thing, etc.   \n",
       "10843        free              not attached to sth or trapped by sth   \n",
       "8592   elementary                             of the most basic kind   \n",
       "\n",
       "                                                sentence  \n",
       "20662   We're not sure whether we still want to proce...  \n",
       "9356      a planned two-storey extension to the hospital  \n",
       "6533           She held the bird gently in cupped hands.  \n",
       "19993              Here are the main points of the news.  \n",
       "15999       If you love each other, why not get married?  \n",
       "...                                                  ...  \n",
       "18702      You seem to have packed a lot into your life!  \n",
       "7130           She denied all knowledge of the incident.  \n",
       "14493   The village itself is pretty, but the surroun...  \n",
       "10843           Pull gently on the free end of the rope.  \n",
       "8592                    the elementary laws of economics  \n",
       "\n",
       "[16015 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c3f969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.971776459569154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['mean'].map(lambda x : len(x.split(' '))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6acf11",
   "metadata": {},
   "source": [
    "# 1.1 templates definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ed70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df979fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6123836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3de6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_template_0(df_row_data):\n",
    "    len_word = len(tokenizer.encode(df_row_data['word'],add_special_tokens=False))\n",
    "    templated_sentence= \" \".join([tokenizer.mask_token for i in range(len_word)]) + \" means \" + df_row_data['mean']\n",
    "    label = df_row_data['word']\n",
    "    return templated_sentence,label\n",
    "\n",
    "def process_data_template_1(df_row_data):\n",
    "    len_mean = len(tokenizer.encode(df_row_data['mean'],add_special_tokens=False))\n",
    "    templated_sentence= df_row_data['word'] + \" means \" + \" \".join([tokenizer.mask_token for i in range(len_mean)])\n",
    "    label=df_row_data['mean']\n",
    "    return templated_sentence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f7dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_template_5(df_row_data):\n",
    "    # The { word } in sentence { example } means { meaning }\n",
    "    len_word = len(tokenizer.encode(df_row_data['word'],add_special_tokens=False))\n",
    "    len_mean = len(tokenizer.encode(df_row_data['mean'],add_special_tokens=False))\n",
    "    len_label_1 = len(tokenizer.encode(\"The word \",add_special_tokens=False))\n",
    "    len_label_2 = len(tokenizer.encode(\" in sentence '\",add_special_tokens=False))\n",
    "    \n",
    "    word_list = df_row_data['sentence'].split(' ')\n",
    "    # 先构建原始句子\n",
    "    template5_sentence = \"The word \" + \" \".join([tokenizer.mask_token for i in range(len_word)]) + \" in sentence '\"\n",
    "    label = \" \".join([tokenizer.mask_token for i in range(len_label_1)]) +  \" \" + df_row_data['word'] + \" \" + \" \".join([tokenizer.mask_token for i in range(len_label_2)])\n",
    "    sub_sentence_list = []\n",
    "    temp_word_list = []\n",
    "    label_word_list = []\n",
    "    #temp_w_num = 0\n",
    "    #print(word_list)\n",
    "    #print()\n",
    "    # 做这个处理的原因是，有可能一个句子里存在多个目标词\n",
    "    for w in word_list:\n",
    "        if re.match(df_row_data['word'],w) is not None:\n",
    "            #print(\"match\")\n",
    "            #print(temp_word_list)\n",
    "            label_word_list.append(w)\n",
    "            if temp_word_list == []:\n",
    "                continue\n",
    "            sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "            temp_word_list = []\n",
    "            #temp_w_num += 1\n",
    "        else:\n",
    "            temp_word_list.append(w)\n",
    "            #print(temp_word_list)\n",
    "    sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "    #print(label_word_list)\n",
    "    # 这样做的目的是，有些词的不同形式可能存在不同的tokenizer的大小\n",
    "    \n",
    "    w_num = sum([len(tokenizer.encode(i,add_special_tokens=False)) for i in label_word_list]) #标签中的词被处理后的id数量\n",
    "    sent_num = 0 # template句子中的词的数量\n",
    "    temp_w_num = 0 # 用于存储每个词的id累积到什么程度\n",
    "    #temp_sent_num = 0\n",
    "    #print(sub_sentence_list)\n",
    "    label += \" \"\n",
    "    # 把词都填回去\n",
    "    for sub_sent in sub_sentence_list:\n",
    "        template5_sentence += sub_sent\n",
    "        template5_sentence += \" \"\n",
    "        temp_sent_num = len(tokenizer.encode(sub_sent,add_special_tokens=False))\n",
    "        label += \" \".join([tokenizer.mask_token for i in range(temp_sent_num)])\n",
    "        sent_num += temp_sent_num\n",
    "        if temp_w_num < w_num:\n",
    "            w = label_word_list.pop(0)\n",
    "            temp_w_num += len(tokenizer.encode(w,add_special_tokens=False))\n",
    "            template5_sentence += \" \".join([tokenizer.mask_token for i in range(temp_w_num)])\n",
    "            template5_sentence += \" \"\n",
    "            label += \" \"\n",
    "            label += w\n",
    "    label += \" \"\n",
    "    len_label_3 = len(tokenizer.encode(\"' means '\",add_special_tokens=False))\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_label_3)])\n",
    "    label += \" \"\n",
    "    label += df_row_data['mean']\n",
    "    label += \" \"\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len(tokenizer.encode(\"'.\",add_special_tokens=False)))])\n",
    "    #print(sub_sentence_list)\n",
    "    # 组成最终的句子\n",
    "    template5_sentence = template5_sentence + \"' means '\" + \" \".join([tokenizer.mask_token for i in range(len_mean)]) + \"'.\"\n",
    "    return template5_sentence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233fd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_template6(df_row_data):\n",
    "    #{ meaning } is the meaning for {word} in {example}\n",
    "    label = \"\"\n",
    "    \n",
    "    sub_temp_0 = \"'\"\n",
    "    len_sub_temp_0 = len(tokenizer.encode(sub_temp_0,add_special_tokens=False))\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_0)])\n",
    "    \n",
    "    len_mean = len(tokenizer.encode(df_row_data['mean'],add_special_tokens=False))\n",
    "    label += \" \"\n",
    "    label += df_row_data['mean']\n",
    "    \n",
    "    sub_temp_1 = \"' is the meaning for word '\"\n",
    "    len_sub_temp_1 = len(tokenizer.encode(sub_temp_1,add_special_tokens=False))\n",
    "    template6_sentence =  sub_temp_0 + \" \".join([tokenizer.mask_token for i in range(len_mean)]) + sub_temp_1 # 拼接模板后的子句\n",
    "    label += \" \"\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_1)])\n",
    "    \n",
    "    len_word = len(tokenizer.encode(df_row_data['word'],add_special_tokens=False))\n",
    "    template6_sentence += \" \".join([tokenizer.mask_token for i in range(len_word)])\n",
    "    label += \" \"\n",
    "    label += df_row_data['word']\n",
    "    \n",
    "    sub_temp_2 = \"' in sentence '\"\n",
    "    len_sub_temp_2 = len(tokenizer.encode(sub_temp_2,add_special_tokens=False))\n",
    "    template6_sentence += sub_temp_2#\"\n",
    "    label += \" \"\n",
    "    label +=  \" \".join([tokenizer.mask_token for i in range(len_sub_temp_2)])\n",
    "    \n",
    "    #加入句子\n",
    "    word_list = df_row_data['sentence'].split(' ')\n",
    "    \n",
    "    sub_sentence_list = []\n",
    "    temp_word_list = []\n",
    "    label_word_list = []\n",
    "\n",
    "    # 做这个处理的原因是，有可能一个句子里存在多个目标词\n",
    "    for w in word_list:\n",
    "        if re.match(df_row_data['word'],w) is not None:\n",
    "            #print(\"match\")\n",
    "            #print(temp_word_list)\n",
    "            label_word_list.append(w)\n",
    "            if temp_word_list == []:\n",
    "                continue\n",
    "            sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "            temp_word_list = []\n",
    "            #temp_w_num += 1\n",
    "        else:\n",
    "            temp_word_list.append(w)\n",
    "            #print(temp_word_list)\n",
    "    sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "    \n",
    "    w_num = sum([len(tokenizer.encode(i,add_special_tokens=False)) for i in label_word_list]) #标签中的词被处理后的id数量\n",
    "    sent_num = 0 # template句子中的词的数量\n",
    "    temp_w_num = 0 # 用于存储每个词的id累积到什么程度\n",
    "    #temp_sent_num = 0\n",
    "    #print(sub_sentence_list)\n",
    "    label += \" \"\n",
    "    \n",
    "    # 把词都填回去\n",
    "    #print(len_mean)\n",
    "    for sub_sent in sub_sentence_list:\n",
    "        template6_sentence += sub_sent\n",
    "        template6_sentence += \" \"\n",
    "        temp_sent_num = len(tokenizer.encode(sub_sent,add_special_tokens=False))\n",
    "        label += \" \".join([tokenizer.mask_token for i in range(temp_sent_num)])\n",
    "        sent_num += temp_sent_num\n",
    "        if temp_w_num < w_num:\n",
    "            w = label_word_list.pop(0)\n",
    "            temp_w_num += len(tokenizer.encode(w,add_special_tokens=False))\n",
    "            #template6_sentence += \" \"\n",
    "            template6_sentence += \" \".join([tokenizer.mask_token for i in range(temp_w_num)])\n",
    "            template6_sentence += \" \"\n",
    "            label += \" \"\n",
    "            label += w\n",
    "            label += \" \"\n",
    "    sub_temp_3 = \"'\"\n",
    "    template6_sentence += sub_temp_3\n",
    "    return template6_sentence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004a4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_template_7(df_row_data):\n",
    "    # the word {word} means {meaning}, for example in sentence {sentence}.\n",
    "    # 先是一些预备的东西，用于连接句子\n",
    "    sub_temp_0 = \"The word '\"\n",
    "    len_sub_temp_0 = len(tokenizer.encode(sub_temp_0,add_special_tokens=False))\n",
    "    len_word = len(tokenizer.encode(df_row_data['word'],add_special_tokens=False))\n",
    "    \n",
    "    sub_temp_1 = \"' means '\"\n",
    "    len_sub_temp_1 = len(tokenizer.encode(sub_temp_1,add_special_tokens=False))\n",
    "    \n",
    "    len_mean = len(tokenizer.encode(df_row_data['mean'],add_special_tokens=False))\n",
    "    \n",
    "    sub_temp_2 = \"', for example in sentence '\"\n",
    "    len_sub_temp_2 = len(tokenizer.encode(sub_temp_2,add_special_tokens=False))\n",
    "\n",
    "    sub_temp_3 = \"'\"\n",
    "    len_sub_temp_3 = len(tokenizer.encode(sub_temp_3,add_special_tokens=False))\n",
    "\n",
    "    template_sentence_7 = \"\"\n",
    "    label = \"\"\n",
    "    # 开始拼接句子\n",
    "    # 先把简单的拼上去\n",
    "    template_sentence_7 += sub_temp_0\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_0)])\n",
    "    label += \" \"\n",
    "    template_sentence_7 += \" \".join([tokenizer.mask_token for i in range(len_word)])\n",
    "    label += df_row_data['word']\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_7 += sub_temp_1\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_1)])\n",
    "    label += \" \"\n",
    "    template_sentence_7 += \" \".join([tokenizer.mask_token for i in range(len_mean)])\n",
    "    label += df_row_data['mean']\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_7 += sub_temp_2\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_2)])\n",
    "    label += \" \"\n",
    "    #template_sentence_7 += \n",
    "    \n",
    "    #加入句子\n",
    "    word_list = df_row_data['sentence'].split(' ')\n",
    "    \n",
    "    sub_sentence_list = []\n",
    "    temp_word_list = []\n",
    "    label_word_list = []\n",
    "\n",
    "    # 做这个处理的原因是，有可能一个句子里存在多个目标词\n",
    "    for w in word_list:\n",
    "        if re.match(df_row_data['word'],w) is not None:\n",
    "            #print(\"match\")\n",
    "            #print(temp_word_list)\n",
    "            label_word_list.append(w)\n",
    "            if temp_word_list == []:\n",
    "                continue\n",
    "            sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "            temp_word_list = []\n",
    "            #temp_w_num += 1\n",
    "        else:\n",
    "            temp_word_list.append(w)\n",
    "            #print(temp_word_list)\n",
    "    sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "    \n",
    "    w_num = sum([len(tokenizer.encode(i,add_special_tokens=False)) for i in label_word_list]) #标签中的词被处理后的id数量\n",
    "    sent_num = 0 # template句子中的词的数量\n",
    "    temp_w_num = 0 # 用于存储每个词的id累积到什么程度\n",
    "    #temp_sent_num = 0\n",
    "    #print(sub_sentence_list)\n",
    "    label += \" \"\n",
    "    \n",
    "    # 把词都填回去\n",
    "    #print(len_mean)\n",
    "    for sub_sent in sub_sentence_list:\n",
    "        template_sentence_7 += sub_sent\n",
    "        template_sentence_7 += \" \"\n",
    "        temp_sent_num = len(tokenizer.encode(sub_sent,add_special_tokens=False))\n",
    "        label += \" \".join([tokenizer.mask_token for i in range(temp_sent_num)])\n",
    "        sent_num += temp_sent_num\n",
    "        if temp_w_num < w_num:\n",
    "            w = label_word_list.pop(0)\n",
    "            temp_w_num += len(tokenizer.encode(w,add_special_tokens=False))\n",
    "            #template6_sentence += \" \"\n",
    "            template_sentence_7 += \" \".join([tokenizer.mask_token for i in range(temp_w_num)])\n",
    "            template_sentence_7 += \" \"\n",
    "            label += \" \"\n",
    "            label += w\n",
    "            label += \" \"\n",
    "    template_sentence_7 += sub_temp_3\n",
    "    label += \" \"\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_3)])\n",
    "    return template_sentence_7,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b03809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_template_8(df_row_data):\n",
    "    # In sentence {sentence}, the interpretation of the {word} is {meaning}\n",
    "    sub_temp_0 = \"In sentence '\"\n",
    "    sub_temp_1 = \"', the interpretation of the word '\"\n",
    "    sub_temp_2 = \"' is '\"\n",
    "    sub_temp_3 = \"'.\"\n",
    "    \n",
    "    len_sub_temp_0 = len(tokenizer.encode(sub_temp_0,add_special_tokens=False))\n",
    "    len_sub_temp_1 = len(tokenizer.encode(sub_temp_1,add_special_tokens=False))\n",
    "    len_sub_temp_2 = len(tokenizer.encode(sub_temp_2,add_special_tokens=False))\n",
    "    len_sub_temp_3 = len(tokenizer.encode(sub_temp_3,add_special_tokens=False))\n",
    "    \n",
    "    len_word = len(tokenizer.encode(df_row_data['word'],add_special_tokens=False))\n",
    "    len_mean = len(tokenizer.encode(df_row_data['mean'],add_special_tokens=False))\n",
    "    \n",
    "    template_sentence_8 = \"\"\n",
    "    label = \" \"\n",
    "    \n",
    "    template_sentence_8 += sub_temp_0\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_0)])\n",
    "    \n",
    "    #加入句子\n",
    "    word_list = df_row_data['sentence'].split(' ')\n",
    "    \n",
    "    sub_sentence_list = []\n",
    "    temp_word_list = []\n",
    "    label_word_list = []\n",
    "\n",
    "    # 做这个处理的原因是，有可能一个句子里存在多个目标词\n",
    "    for w in word_list:\n",
    "        if re.match(df_row_data['word'],w) is not None:\n",
    "            #print(\"match\")\n",
    "            #print(temp_word_list)\n",
    "            label_word_list.append(w)\n",
    "            if temp_word_list == []:\n",
    "                continue\n",
    "            sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "            temp_word_list = []\n",
    "            #temp_w_num += 1\n",
    "        else:\n",
    "            temp_word_list.append(w)\n",
    "            #print(temp_word_list)\n",
    "    sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "    \n",
    "    w_num = sum([len(tokenizer.encode(i,add_special_tokens=False)) for i in label_word_list]) #标签中的词被处理后的id数量\n",
    "    sent_num = 0 # template句子中的词的数量\n",
    "    temp_w_num = 0 # 用于存储每个词的id累积到什么程度\n",
    "    #temp_sent_num = 0\n",
    "    #print(sub_sentence_list)\n",
    "    label += \" \"\n",
    "    \n",
    "    # 把词都填回去\n",
    "    #print(len_mean)\n",
    "    for sub_sent in sub_sentence_list:\n",
    "        template_sentence_8 += sub_sent\n",
    "        template_sentence_8 += \" \"\n",
    "        temp_sent_num = len(tokenizer.encode(sub_sent,add_special_tokens=False))\n",
    "        label += \" \".join([tokenizer.mask_token for i in range(temp_sent_num)])\n",
    "        sent_num += temp_sent_num\n",
    "        if temp_w_num < w_num:\n",
    "            w = label_word_list.pop(0)\n",
    "            temp_w_num += len(tokenizer.encode(w,add_special_tokens=False))\n",
    "            #template6_sentence += \" \"\n",
    "            template_sentence_8 += \" \".join([tokenizer.mask_token for i in range(temp_w_num)])\n",
    "            template_sentence_8 += \" \"\n",
    "            label += \" \"\n",
    "            label += w\n",
    "            label += \" \"    \n",
    "    \n",
    "    template_sentence_8 += sub_temp_1\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_1)])\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_8 += \" \".join([tokenizer.mask_token for i in range(len_word)])\n",
    "    label += df_row_data['word']\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_8 += sub_temp_2\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_2)])\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_8 += \" \".join([tokenizer.mask_token for i in range(len_mean)])\n",
    "    label += df_row_data['mean']\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_8 += sub_temp_3\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_3)])\n",
    "    return template_sentence_8,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_template_9(df_row_data):\n",
    "    # In the sentence {sentence}, word that means {meaning} is {word}\n",
    "    sub_temp_0 = \"In the sentence '\"\n",
    "    sub_temp_1 = \"', word that means '\"\n",
    "    sub_temp_2 = \"' is '\"\n",
    "    sub_temp_3 = \"'.\"\n",
    "    \n",
    "    \n",
    "    len_sub_temp_0 = len(tokenizer.encode(sub_temp_0,add_special_tokens=False))\n",
    "    len_sub_temp_1 = len(tokenizer.encode(sub_temp_1,add_special_tokens=False))\n",
    "    len_sub_temp_2 = len(tokenizer.encode(sub_temp_2,add_special_tokens=False))\n",
    "    len_sub_temp_3 = len(tokenizer.encode(sub_temp_3,add_special_tokens=False))\n",
    "    \n",
    "    len_word = len(tokenizer.encode(df_row_data['word'],add_special_tokens=False))\n",
    "    len_mean = len(tokenizer.encode(df_row_data['mean'],add_special_tokens=False))\n",
    "    \n",
    "    template_sentence_9 = \"\"\n",
    "    label = \" \"\n",
    "    \n",
    "    template_sentence_9 += sub_temp_0\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_0)])\n",
    "    label += \" \"\n",
    "    \n",
    "    #加入句子\n",
    "    word_list = df_row_data['sentence'].split(' ')\n",
    "    \n",
    "    sub_sentence_list = []\n",
    "    temp_word_list = []\n",
    "    label_word_list = []\n",
    "\n",
    "    # 做这个处理的原因是，有可能一个句子里存在多个目标词\n",
    "    for w in word_list:\n",
    "        if re.match(df_row_data['word'],w) is not None:\n",
    "            #print(\"match\")\n",
    "            #print(temp_word_list)\n",
    "            label_word_list.append(w)\n",
    "            if temp_word_list == []:\n",
    "                continue\n",
    "            sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "            temp_word_list = []\n",
    "            #temp_w_num += 1\n",
    "        else:\n",
    "            temp_word_list.append(w)\n",
    "            #print(temp_word_list)\n",
    "    sub_sentence_list.append(\" \".join(temp_word_list))\n",
    "    \n",
    "    w_num = sum([len(tokenizer.encode(i,add_special_tokens=False)) for i in label_word_list]) #标签中的词被处理后的id数量\n",
    "    sent_num = 0 # template句子中的词的数量\n",
    "    temp_w_num = 0 # 用于存储每个词的id累积到什么程度\n",
    "    #temp_sent_num = 0\n",
    "    #print(sub_sentence_list)\n",
    "    label += \" \"\n",
    "    \n",
    "    # 把词都填回去\n",
    "    #print(len_mean)\n",
    "    for sub_sent in sub_sentence_list:\n",
    "        template_sentence_9 += sub_sent\n",
    "        template_sentence_9 += \" \"\n",
    "        temp_sent_num = len(tokenizer.encode(sub_sent,add_special_tokens=False))\n",
    "        label += \" \".join([tokenizer.mask_token for i in range(temp_sent_num)])\n",
    "        sent_num += temp_sent_num\n",
    "        if temp_w_num < w_num:\n",
    "            w = label_word_list.pop(0)\n",
    "            temp_w_num += len(tokenizer.encode(w,add_special_tokens=False))\n",
    "            #template6_sentence += \" \"\n",
    "            template_sentence_9 += \" \".join([tokenizer.mask_token for i in range(temp_w_num)])\n",
    "            template_sentence_9 += \" \"\n",
    "            label += \" \"\n",
    "            label += w\n",
    "            label += \" \"    \n",
    "    \n",
    "    label += \" \"\n",
    "    template_sentence_9 += sub_temp_1\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_1)])\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_9 += \" \".join([tokenizer.mask_token for i in range(len_mean)])\n",
    "    label += df_row_data['mean']\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_9 += sub_temp_2\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_2)])\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_9 += \" \".join([tokenizer.mask_token for i in range(len_word)])\n",
    "    label += df_row_data['word']\n",
    "    label += \" \"\n",
    "    \n",
    "    template_sentence_9 += sub_temp_3\n",
    "    label += \" \".join([tokenizer.mask_token for i in range(len_sub_temp_3)])\n",
    "    label += \" \"\n",
    "    \n",
    "    return template_sentence_9,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c2a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_row_context_func(df_row_data,template_id):\n",
    "    # 直接是词语对应意思的方式\n",
    "    if template_id == 0:\n",
    "        templated_sentence,label = process_data_template_0(df_row_data)\n",
    "    elif template_id == 1:\n",
    "        templated_sentence,label = process_data_template_1(df_row_data)\n",
    "    elif template_id == 2:\n",
    "        # 根据上下文，生成词语和意思,在句子中间加带有括号的解释\n",
    "        word_list = df_row_data['sentence'].split(' ')\n",
    "        templated_sentence_list = []\n",
    "        label_list = []\n",
    "        for w in word_list:\n",
    "            if tokenizer(w,add_special_tokens=False) == tokenizer(df_row_data['word'],add_special_tokens=False):\n",
    "                append_mean = w + \" ( which means {} )\".format(df_row_data['mean'])\n",
    "                #label_list.append(w)\n",
    "                label_list.append(append_mean)\n",
    "                templated_sentence_list += [\"[MASK]\"] * len(tokenizer.encode(append_mean,add_special_tokens=False))\n",
    "            else:\n",
    "                templated_sentence_list.append(w)\n",
    "                #label_list.append(\"[MASK]\")\n",
    "        templated_sentence = \" \".join(templated_sentence_list)\n",
    "        label = \" \".join(label_list)\n",
    "    elif template_id == 3:\n",
    "        # 根据上下文，生成词语和意思,在句子中间加带有括号的解释,括号里的更少了\n",
    "        word_list = df_row_data['sentence'].split(' ')\n",
    "        templated_sentence_list = []\n",
    "        label_list = []\n",
    "        for w in word_list:\n",
    "            if tokenizer(w,add_special_tokens=False) == tokenizer(df_row_data['word'],add_special_tokens=False):\n",
    "                append_mean = w + \" ( {} )\".format(df_row_data['mean'])\n",
    "                #label_list.append(w)\n",
    "                label_list.append(append_mean)\n",
    "                templated_sentence_list += [\"[MASK]\"] * len(tokenizer.encode(append_mean,add_special_tokens=False))\n",
    "            else:\n",
    "                templated_sentence_list.append(w)\n",
    "                #label_list.append(\"[MASK]\")\n",
    "        templated_sentence = \" \".join(templated_sentence_list)\n",
    "        label = \" \".join(label_list)\n",
    "    elif template_id == 4:\n",
    "        # 根据上下文，生成词语和意思,但是改变了生成方式\n",
    "        word_list = df_row_data['sentence'].split(' ')\n",
    "        templated_sentence_list = []\n",
    "        label_list = []\n",
    "        for w in word_list:\n",
    "            if tokenizer(w,add_special_tokens=False) == tokenizer(df_row_data['word'],add_special_tokens=False):\n",
    "                label_list.append(w)\n",
    "                templated_sentence_list.append(\"[MASK]\")\n",
    "            else:\n",
    "                templated_sentence_list.append(w)\n",
    "        append_mean = \", which {} means {} .\".format(df_row_data['word'],df_row_data['mean'])\n",
    "        label_list.append(append_mean)\n",
    "        templated_sentence_list += [\"[MASK]\"] * len(tokenizer.encode(append_mean,add_special_tokens=False))\n",
    "        templated_sentence = \" \".join(templated_sentence_list)\n",
    "        label = \" \".join(label_list)\n",
    "    elif template_id == 5:\n",
    "        templated_sentence,label = process_data_template_5(df_row_data)\n",
    "    elif template_id == 6:\n",
    "        templated_sentence,label = process_data_template6(df_row_data)\n",
    "    elif template_id == 7:\n",
    "        templated_sentence,label = process_data_template_7(df_row_data)\n",
    "    elif template_id == 8:\n",
    "        templated_sentence,label = process_data_template_8(df_row_data)\n",
    "    elif template_id == 9:\n",
    "        templated_sentence,label = process_data_template_9(df_row_data)\n",
    "    else:\n",
    "        templated_sentence = \"\"\n",
    "        label = \"\"\n",
    "    return templated_sentence,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f202122",
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_sentence,label = process_df_row_context_func(train.iloc[77],template_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6c12b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The word '[MASK]' means '[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]', for example in sentence 'More and more travelers are looking for bed and breakfast [MASK] in private homes. '\",\n",
       " '[MASK] [MASK] [MASK] accommodation [MASK] [MASK] [MASK] somewhere to live or stay, often also providing food or other services [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]  [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] accommodations [MASK] [MASK] [MASK] [MASK] [MASK]')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templated_sentence,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b96098",
   "metadata": {},
   "source": [
    "# 1.2 process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc526095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples,max_length=24,template_id = template_id):\n",
    "    templated_sentence,label = process_df_row_context_func(examples,template_id)\n",
    "    tokenized_sentence = tokenizer(templated_sentence,max_length = max_length, padding='max_length', truncation=True)\n",
    "    label = tokenizer.encode(label,add_special_tokens=False)\n",
    "    labels = []\n",
    "    j = 0\n",
    "    for i in tokenized_sentence.input_ids:\n",
    "        if i != 103:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(label[j])\n",
    "            j += 1\n",
    "    #labels = [-100 for i in tokenized_sentence.input_ids if i != 103 else label[0]]\n",
    "    tokenized_sentence['labels'] = labels\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61e7eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2773, 1005, 103, 1005, 2965, 1005, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, 103, -100, -100, -100, 103, 103, 10838, 103, 103, 103, 2000, 3613, 2725, 2358, 2232, 2008, 2038, 2525, 2042, -100]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ace80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(train)\n",
    "dataset_test = Dataset.from_pandas(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d37d3f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787a2edc96d24e8ab2c8f635e36c37d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5694c8c16a3141ed948fe5bfa121ea0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_datasets = dataset_train.map(tokenize_function, batched=False)\n",
    "tokenized_test_datasets = dataset_test.map(tokenize_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2abd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_datasets = tokenized_train_datasets.remove_columns(['word','mean','sentence','__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90829e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 2773, 1005, 103, 1005, 2965, 1005, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, 103, -100, -100, -100, 103, 103, 10838, 103, 103, 103, 2000, 3613, 2725, 2358, 2232, 2008, 2038, 2525, 2042, -100]}\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_train_datasets:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59815c11",
   "metadata": {},
   "source": [
    "# 1.3 Load model  and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f27c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b81a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import (\n",
    "    BertPreTrainedModel,\n",
    "    BertModel\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    MaskedLMOutput\n",
    ")\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertOnlyMLMHead\n",
    ")\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5be3972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMaskedLM(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if config.is_decoder:\n",
    "            logger.warning(\n",
    "                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n",
    "                \"bi-directional self-attention.\"\n",
    "            )\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
    "            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        \n",
    "        ## my outputs\n",
    "        index_mask = (labels>0).nonzero(as_tuple=True)\n",
    "        predictions = prediction_scores[index_mask]\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=predictions.argmax(axis=-1),\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        effective_batch_size = input_shape[0]\n",
    "\n",
    "        #  add a dummy token\n",
    "        if self.config.pad_token_id is None:\n",
    "            raise ValueError(\"The PAD token should be defined for generation\")\n",
    "\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
    "        dummy_token = torch.full(\n",
    "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e475e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at F:\\preTrainedModels\\bert-base-uncased\\ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a11cc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = model(\n",
    "    input_ids=torch.LongTensor([i['input_ids']]),\n",
    "    token_type_ids = torch.LongTensor(i['token_type_ids']),\n",
    "    attention_mask=torch.LongTensor([i['attention_mask']]),\n",
    "    labels=torch.LongTensor([i['labels']]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adf7744f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(11.7715, grad_fn=<NllLossBackward>), logits=tensor([1037, 2000, 1010, 1005, 1005, 1010, 1010, 1010, 1010, 1010, 1010, 1010,\n",
       "        1005, 1005, 1005, 1012]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69c49be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc52262",
   "metadata": {},
   "source": [
    "# 1.4 Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "775c0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dd42178",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    #eval_steps = 1,\n",
    "    learning_rate         = lr,\n",
    "    save_strategy='epoch',\n",
    "    weight_decay          = wd,\n",
    "    num_train_epochs      = epoch,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size  = batch_size,\n",
    "    #label_smoothing_factor = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "840918ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_datasets,\n",
    "    eval_dataset =tokenized_test_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff32dd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word, mean, __index_level_0__, sentence. If word, mean, __index_level_0__, sentence are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1779\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='223' max='223' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [223/223 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 11.745624542236328,\n",
       " 'eval_runtime': 4.4605,\n",
       " 'eval_samples_per_second': 398.832,\n",
       " 'eval_steps_per_second': 49.994}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5e52d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c411b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
